
## CFIIBMR
Akash has worked on the process of migrating the data from Microsoft PDW via on premise Hadoop Utility to tables/procedures- over Azure SQL-DW. Akash also worked on Data validation and procedure optimization developed using Pyspark.
• Migrated tables/procedures into Azure SQL-DW.
• Processed the data in Spark using pySpark over on prem Hadoop cluster.
• Worked on enhancing the performance of the queries.
• Created Automation tool for simple table migration end to end with new table creation over SQL-DW.
• Data Validation through PY-Spark jobs over tableau dashboard.
• Exported data to Azure SQL-DW to generate reports.
Skills/Domain: Hadoop, Hive, Python, Spark, SQL, Azure, Azure Sql dw, Microsoft PDW, AWS


## P101
Akash worked on the process of migrating the data from multiple SQL-Server via Azure Data Factory to Azure Lake and process using Spark to result in Analysis cube under Azure SQL-DB.
• Migrated tables into Azure Lake Storage.
• Processed the data in Spark using pySpark over Databricks.
• Worked on enhancing the performance of the queries.
• Created ADF Pipeline for end to end flow from migration to analytics.
• Exported data to Azure SQL-DB to generate Analysis(SSIS) Cube.
Skills/Domain: Big Data/Cloud


## ELIPS
Equipment Lifecycle and Integration of Parts and Service is a part of system wherein all the data from manufacturing domain which is in XML format is processed in Hadoop framework. It includes analysis sales and service information of various machine parts which helps in understanding the growth as well as progress of organization and service respectively.
• Analyse Client Systems and gather requirements.
• XML parsing using MR , XSD and XSLT.
• Dump the data into hive tables.
• Transfer data into traditional RDBMS – Netezza after certain hive processing using SQOOP
• Schedule the entire flow in Oozie.

Skills/Domain: Big data

## SA5
Service administrator is an analytics solution for analysing json logs generated while browsing the service manuals in order to find out the most problematic areas. The service manuals are actually websites which contains complete solution of the problems that customers normally might face for which they need not call customer care but can find the solution themselves by browsing the manual.
• Analyse Client Systems and gather requirements.
• Analysing web logs which are in json format.
• Developed complex hive queries to merge the data from DB2 for actual report generation.
• Developed scripts for archiving files in Hadoop archive format.
• Exporting data to postgre using sqoop which is further used by visualization tools for report generation.
• Scheduling the actions using Oozie.

Skills/Domain: Big data

## Warranty
Warranty is a process of migrating the data from DB2  via Informatica to Redshift and considering performance issues while migrating over cloud.
• Migrated tables into S3
• Processed the data in Hive-EMR using complex queries.
• Worked on enhancing the performance of the queries
• Created Data Pipeline
• Exported data to Redshift using DataPipeline
Skills/Domain: Big data/Cloud

## Combined Table
Combined Table is a complete process of combining the data from various sources so as to reduce the existing load on Netezza and considering performance issues while migrating.
• Migrated Existing Netezza tables into hive
• Processed the data in Hive using complex queries.
• Worked on enhancing the performance of the queries
• Created Oozie workflows
• Wrote UDF for UTF conversion
• Exported data to Netezza using Sqoop

## Experience

###Senior Associate
###Publicis Sapient
Dec 2019 - Present (1 year 11 months +)
• Build automated framework using AWS code build, Docker, Apache Airflow, DBT, and Kedro forprocessing datasets in SCD1 and SCD2 flavors.
• Processed the data in Spark using pySpark over on-prem Hadoop cluster.
• Migrated tables/procedures into Azure SQL-DW.
• Created Automation tool for simple table migration end to end with new table creation over SQL-DW.
• Data Validation through PY-Spark jobs over tableau dashboard.
• Exported data to Azure SQL-DW to generate reports.

###Big Data Developer
###YASH Technologies
Jan 2019 - Dec 2019 (1 year)
• Migrated tables into Azure Lake Storage.
• Processed the data in Spark using pySpark over Databricks.
• Worked on enhancing the performance of the queries.
• Created ADF Pipeline for end-to-end flow from migration to analytics.
• Exported data to Azure SQL-DB to generate Analysis(SSIS) Cube.
• A result oriented professional with 8 years of experience in software development using BigData, Hadoop,

###Technology Analyst
###Infosys
Oct 2013 - Dec 2018 (5 years)
• Analyse Client Systems and gather requirements.
• Analysing weblogs which are in JSON format.
• Developed complex hive queries to merge the data from DB2 for actual report generation.
• Developed scripts for archiving files in Hadoop archive format.
• Exporting data to Postgres using sqoop which is further used by visualization tools for reportgeneration.
• Scheduling the actions using Oozie.

<dl>
<dt>Name</dt>
<dd>Akash</dd>
<dt>Born</dt>
<dd>1991</dd>
<dt>Birthplace</dt>
<dd>Delhi, India</dd>
<dt>Color</dt>
<dd>Green</dd>
</dl>
